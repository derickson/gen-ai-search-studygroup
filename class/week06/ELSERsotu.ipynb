{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELSER the State of the Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure ELSER is deployed and started\n",
    "\n",
    "Full documentation including air gap instructions are here: https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-elser.html\n",
    "\n",
    "But the easiest way to do it is probably in Dev Tools\n",
    "\n",
    "```PUT _ml/trained_models/.elser_model_1```\n",
    "```json\n",
    "{\n",
    "  \"input\": {\n",
    "\t\"field_names\": [\"text_field\"]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "and then after the ELSER download from elastic's cloud repo is complete\n",
    "\n",
    "```POST _ml/trained_models/.elser_model_1/deployment/_start?deployment_id=for_search```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our State of the union speeches\n",
    "\n",
    "Let's clean up the text a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install icecream\n",
    "\n",
    "import pickle\n",
    "from icecream import ic\n",
    "\n",
    "\n",
    "PICKLE_FILE = \"./STATE_OF_THE_UNION.pickle\"\n",
    "\n",
    "speeches = None\n",
    "with open(PICKLE_FILE, 'rb') as f:\n",
    "    speeches = pickle.load(f)\n",
    "\n",
    "\n",
    "print(speeches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility code\n",
    "def write_strings_to_file(strings, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for line in strings:\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "def write_docs_to_file(docs, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for doc in docs:\n",
    "            file.write(doc.page_content + '\\n\\n')\n",
    "\n",
    "import json\n",
    "# pretty printing JSON objects\n",
    "def json_pretty(input_object):\n",
    "  print(json.dumps(input_object, indent=4))\n",
    "\n",
    "\n",
    "import textwrap\n",
    "# wrap text when printing, because colab scrolls output to the right too much\n",
    "def wrap_text(text, width):\n",
    "    wrapped_text = textwrap.wrap(text, width)\n",
    "    return '\\n'.join(wrapped_text)\n",
    "\n",
    "def print_light_blue(text):\n",
    "    print(f'\\033[94m{text}\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from icecream import ic\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = None\n",
    "\n",
    "if 'ELASTIC_CLOUD_ID' in os.environ:\n",
    "  es = Elasticsearch(\n",
    "    cloud_id=os.environ['ELASTIC_CLOUD_ID'],\n",
    "    basic_auth=(os.environ['ELASTIC_USER'], os.environ['ELASTIC_PASSWORD']),\n",
    "    request_timeout=30\n",
    "  )\n",
    "elif 'ELASTIC_URL' in os.environ:\n",
    "  es = Elasticsearch(\n",
    "    os.environ['ELASTIC_URL'],\n",
    "    basic_auth=(os.environ['ELASTIC_USER'], os.environ['ELASTIC_PASSWORD']),\n",
    "    request_timeout=30\n",
    "  )\n",
    "else:\n",
    "  print(\"env needs to set either ELASTIC_CLOUD_ID or ELASTIC_URL\")\n",
    "\n",
    "if es:\n",
    "    ic(es.info()['tagline']) # should return cluster info\n",
    "    version = ic(es.info()['version']['number'])\n",
    "    if version < \"8.13.0\" :\n",
    "       print(\"WARNING THIS LAB ASSUMES You are on Elasticsearch Version 8.13 or higher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q langchain==0.1.13 langchain-elasticsearch==0.1.1\n",
    "\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "\n",
    "index_name = \"sotu_chunks_elser\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Chunk the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from elasticsearch.exceptions import NotFoundError\n",
    "from langchain.docstore.document import Document\n",
    "import re\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap  = 0,\n",
    "    length_function = len,\n",
    "    is_separator_regex = True,\n",
    ")\n",
    "\n",
    "firstSpeech = speeches[0]\n",
    "\n",
    "\n",
    "original_dict = {'key1': 'value1', 'key2': ['list', 'elements'], 'key3': {'nested': 'dict'}}\n",
    "deep_copied_dict = copy.deepcopy(original_dict)\n",
    "\n",
    "## this function processes a speech and turs it into Langchain Documents\n",
    "def processSpeech(speech):\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    date = speech[\"date\"]\n",
    "    date_iso = speech[\"date_iso\"]\n",
    "    url = speech[\"url\"]\n",
    "    administration = speech[\"administration\"]\n",
    "    sotu_id = f\"{administration}-{date_iso}\"\n",
    "    meta = {\n",
    "            # \"chunk\": i,\n",
    "            \"date\": date,\n",
    "            \"date_iso\": date_iso,\n",
    "            \"url\": url,\n",
    "            \"administration\": administration,\n",
    "            \"sotu_id\": sotu_id\n",
    "        }    \n",
    "    \n",
    "    # print(f\"Processing {sotu_id}\")\n",
    "\n",
    "    text = speech[\"text\"].strip()\n",
    "    text_chunks = text_splitter.split_text(text)\n",
    "\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        my_meta = copy.deepcopy(meta)\n",
    "        my_meta[\"chunk\"] = i\n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata=my_meta\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    es.indices.delete(index=index_name)\n",
    "except NotFoundError as e:\n",
    "    print(f\"The index '{index_name}' was not found, but that's okay ...moving on\")\n",
    "\n",
    "elastic_vector_search = ElasticsearchStore(\n",
    "    es_connection=es,\n",
    "    index_name=index_name,\n",
    "    strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(model_id=\".elser_model_2_linux-x86_64\")\n",
    ")\n",
    "\n",
    "\n",
    "docs_inserted = 0\n",
    "\n",
    "for speech in tqdm(speeches):\n",
    "    chunkDocuments = processSpeech(speech)\n",
    "\n",
    "    \n",
    "    elastic_vector_search.add_documents(\n",
    "        chunkDocuments,\n",
    "        bulk_kwargs={\n",
    "            \"chunk_size\": 16,\n",
    "            \"max_chunk_bytes\": 200000000\n",
    "        }\n",
    "    )\n",
    "\n",
    "    docs_inserted += len(chunkDocuments)\n",
    "        \n",
    "    elastic_vector_search.client.indices.refresh(index=index_name)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_vector_search.similarity_search(query=\"What did Biden say about the people of Ukraine?\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_vector_search.similarity_search(query=\"What did Clinton say about Ukraine?\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = [{\"term\": {\"metadata.administration.keyword\": \"Clinton\"}}]\n",
    "elastic_vector_search.similarity_search(query=\"What did Clinton say about Ukraine?\", filter=filter, k=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
