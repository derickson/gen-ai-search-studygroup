{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just enough LangChain for RAG\n",
    "\n",
    "I use OpenAI below. You may need to adjust for a different LLM using examples from week 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Elasticsearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from icecream import ic\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = None\n",
    "\n",
    "if 'ELASTIC_CLOUD_ID' in os.environ:\n",
    "  es = Elasticsearch(\n",
    "    cloud_id=os.environ['ELASTIC_CLOUD_ID'],\n",
    "    basic_auth=(os.environ['ELASTIC_USER'], os.environ['ELASTIC_PASSWORD']),\n",
    "    request_timeout=30\n",
    "  )\n",
    "elif 'ELASTIC_URL' in os.environ:\n",
    "  es = Elasticsearch(\n",
    "    os.environ['ELASTIC_URL'],\n",
    "    basic_auth=(os.environ['ELASTIC_USER'], os.environ['ELASTIC_PASSWORD']),\n",
    "    request_timeout=30\n",
    "  )\n",
    "else:\n",
    "  print(\"env needs to set either ELASTIC_CLOUD_ID or ELASTIC_URL\")\n",
    "\n",
    "if es:\n",
    "    ic(es.info()['tagline']) # should return cluster info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a langchain abstraction for ELSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "\n",
    "index_name = \"sotu_chunks_elser\"\n",
    "\n",
    "es_elser = ElasticsearchStore(\n",
    "    es_connection=es,\n",
    "    index_name=index_name,\n",
    "    strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(model_id=\".elser_model_2_linux-x86_64\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a simple semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_input = \"what did the president say about the resolve of the Ukrainian people?\"\n",
    "\n",
    "best_result = ic(es_elser.similarity_search(human_input, k=1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the LLM ready\n",
    "\n",
    "This is likely the part you'll have to change if you are not using OpenAI.  Use one of the ones you got working in week 4 and note you need the \"Chat\" model from Langchain for this to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "def load_openai_llm():\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    default_model = \"gpt-3.5-turbo\"\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0.3,\n",
    "        model=default_model\n",
    "    )\n",
    "    return llm\n",
    "LLM=load_openai_llm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM.invoke(\"The current prime minister of the United Kingdom in 2023 is \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain PromptTemplate\n",
    "\n",
    "LangChain prompts are templates in the following format, letting you bring your own variables retrieved from the structured and unstrutured documents we indexed into Elasticsearch\n",
    "\n",
    "The following code composes a prompt that could be used against OpenAI\n",
    "\n",
    "If you are using LLama2 or another LLM that requires special tokens to separate the system prompt, you'll need to add those yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"You are a helpful AI Chatbot that answers questions \n",
    "about past Presidential State of the Union addresses in short responses using only the following provided context.\n",
    "If you can't answer the question using the following context say \"I don't know\".\n",
    "\n",
    "Context: \n",
    "On {date}, President {administration} said the following:\n",
    "{context}\n",
    "\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"date\", \"administration\", \"context\", \"input\"], \n",
    "    template=TEMPLATE\n",
    ")\n",
    "\n",
    "human_input = \"what did the president say about the resolve of the Ukrainian people?\"\n",
    "\n",
    "print(PROMPT.format(\n",
    "    date= best_result.metadata[\"date\"],\n",
    "    administration= best_result.metadata[\"administration\"],\n",
    "    context= best_result.page_content,\n",
    "    input= human_input\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain ELSER retrieval with a filter\n",
    "the chunks have metadata, a fitler will make retrieval more accurate. the chunks don't spell out which president is speaking in the text field, so mentions of the president by name in the question can throw off ELSER. ELSER may gravitate towards the introductions where the president mentions their spouse or a former president (with same last name) in the building etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "index_name = \"sotu_chunks_elser\"\n",
    "\n",
    "es_elser = ElasticsearchStore(\n",
    "    es_connection=es,\n",
    "    index_name=index_name,\n",
    "    strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(model_id=\".elser_model_2_linux-x86_64\")\n",
    ")\n",
    "\n",
    "def whatDidPresSay(human_input, administration=None, k=1, vebose=False):\n",
    "\n",
    "    filter = [{\"term\": {\"metadata.administration.keyword\": administration}}] if administration else None\n",
    "    best_result = es_elser.similarity_search(human_input, filter = filter, k=k)\n",
    "\n",
    "    return best_result\n",
    "\n",
    "human_input = \"what did the president say about the resolve of the Ukrainian people?\"\n",
    "\n",
    "best_result = whatDidPresSay(human_input=human_input, administration=\"Biden\", vebose=True)\n",
    "\n",
    "best_result[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain the legacy way\n",
    "with LLMChain used by explicit function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "def retrieve_sotu_chain(question, administration):\n",
    "\n",
    "    TEMPLATE = \"\"\"You are a helpful AI Chatbot that answers questions \n",
    "    about past Presidential State of the Union addresses in short responses using only the following provided context.\n",
    "    If you can't answer the question using the following context say \"I don't know\".\n",
    "\n",
    "    Context: \n",
    "    On {date}, President {administration} said the following:\n",
    "    {context}\n",
    "\n",
    "    Human: {input}\n",
    "    AI:\"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(\n",
    "        input_variables=[\"date\", \"administration\", \"context\", \"input\"], \n",
    "        template=TEMPLATE\n",
    "    )\n",
    "\n",
    "    conversation = LLMChain(\n",
    "        prompt=PROMPT,\n",
    "        llm=LLM,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    best_result = whatDidPresSay(human_input=question, administration=administration)[0]\n",
    "\n",
    "    result = conversation.invoke({\n",
    "        \"input\":question, \n",
    "        \"administration\":best_result.metadata[\"administration\"], \n",
    "        \"context\":best_result.page_content, \n",
    "        \"date\":best_result.metadata['date'] \n",
    "    })\n",
    "    return result\n",
    "\n",
    "\n",
    "for pres in ['Biden', 'Trump', 'Obama', \"Bush43\", \"Clinton\"]:\n",
    "    human_input = f\"what did the president say about the economy and economic development?\"\n",
    "    result = retrieve_sotu_chain(question=human_input, administration=pres)[\"text\"]\n",
    "    print(f\"{pres}: {result}\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain with LCEL \"LangChain Expression Language\"\n",
    "Because all tech needs a pipelined query language in 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "TEMPLATE = \"\"\"You are a helpful AI Chatbot that answers questions \n",
    "about past Presidential State of the Union addresses in short responses using only the following provided context.\n",
    "If you can't answer the question using the following context say \"I don't know\".\n",
    "\n",
    "Context: \n",
    "On {date}, President {administration} said the following:\n",
    "{context}\n",
    "\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "        input_variables=[\"date\", \"administration\", \"context\", \"input\"], \n",
    "        template=TEMPLATE\n",
    "    )\n",
    "\n",
    "\n",
    "chain = PROMPT | LLM | output_parser\n",
    "\n",
    "for pres in ['Biden', 'Trump', 'Obama', \"Bush43\", \"Clinton\"]:\n",
    "    question = \"What did the president say about the economy and economic development?\"\n",
    "    best_results = whatDidPresSay(human_input=question, administration=pres)\n",
    "\n",
    "    if best_results != []:\n",
    "        best_result = best_results[0]\n",
    "        \n",
    "        response = chain.invoke({\n",
    "                \"input\":question, \n",
    "                \"administration\":best_result.metadata[\"administration\"], \n",
    "                \"context\":best_result.page_content, \n",
    "                \"date\":best_result.metadata['date'] \n",
    "            })\n",
    "        print(f\"{pres}: {response}\")\n",
    "        print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the filter with AI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "\n",
    "class Presidents(Enum):\n",
    "    BIDEN = \"Biden\"\n",
    "    TRUMP = \"Trump\"\n",
    "    OBAMA = \"Obama\"\n",
    "    BUSH = \"Bush43\"\n",
    "    CLINTON = \"Clinton\"\n",
    "    UNKNOWN = \"Unknown\"\n",
    "parser = EnumOutputParser(enum=Presidents)\n",
    "\n",
    "question = \"What did president Trump say about the economy and economic development\"\n",
    "\n",
    "whichPresTemplate = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"\"\"Out of the following options, Which person is this question about? Answer Unknown if you are not sure\n",
    "    Options: Biden, Trump, Obama, Bush43, Clinton\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "identifyPresChain = whichPresTemplate | LLM | parser\n",
    "\n",
    "presEnum = identifyPresChain.invoke({\"input\":question})\n",
    "\n",
    "presEnum.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"You are a helpful AI Chatbot that answers questions \n",
    "about past Presidential State of the Union addresses in short responses using only the following provided context.\n",
    "If you can't answer the question using the following context say \"I don't know\".\n",
    "\n",
    "Context: \n",
    "On {date}, President {administration} said the following:\n",
    "{context}\n",
    "\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "        input_variables=[\"date\", \"administration\", \"context\", \"input\"], \n",
    "        template=TEMPLATE\n",
    "    )\n",
    "\n",
    "\n",
    "chain = PROMPT | LLM | output_parser\n",
    "\n",
    "\n",
    "question = f\"What did the first black president say about the economy and economic development?\"\n",
    "\n",
    "presEnum = identifyPresChain.invoke({\"input\":question})\n",
    "\n",
    "administration = None if presEnum.value == \"Unknown\" else presEnum.value\n",
    "if administration:\n",
    "    print(f\"> Limiting search to the SOTU addresses of administration: {administration}\")\n",
    "\n",
    "best_results = whatDidPresSay(human_input=question, administration=administration)\n",
    "\n",
    "if best_results != []:\n",
    "    best_result = best_results[0]\n",
    "    \n",
    "    response = chain.invoke({\n",
    "            \"input\":question, \n",
    "            \"administration\":best_result.metadata[\"administration\"], \n",
    "            \"context\":best_result.page_content, \n",
    "            \"date\":best_result.metadata['date'] \n",
    "        })\n",
    "    print(f\"{administration}: {response}\")\n",
    "    print(\"\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
