{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpeanAI Proxy\n",
    "\n",
    "You may have access to a large langugage model through a HTTP Proxy like this one\n",
    "https://github.com/derickson/ExpressOpenAIChatProxy\n",
    "\n",
    "Assuming you know a temporary key for accessing ChatGPT 3.5 through this proxy \n",
    "similar to the Elastic GenAI workshop series here is how to set that up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q openai==1.14.3 python_dotenv\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "## suppress some warnings\n",
    "import warnings, os\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, message=\"on_submit is deprecated.*\")\n",
    "\n",
    "FILE=\"Studygroup\"\n",
    "\n",
    "# workshop environment - this is where you'll enter a key\n",
    "! pip install -qqq git+https://github.com/elastic/notebook-workshop-loader.git@main\n",
    "from notebookworkshoploader import loader\n",
    "from dotenv import load_dotenv\n",
    "loader.load_remote_env(file=FILE, env_url=\"https://notebook-workshop-api-voldmqr2bq-uc.a.run.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, secrets, requests, json\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "#if using the Elastic AI proxy, then generate the correct API key\n",
    "if os.environ['ELASTIC_PROXY'] == \"True\":\n",
    "\n",
    "    if \"OPENAI_API_TYPE\" in os.environ: del os.environ[\"OPENAI_API_TYPE\"]\n",
    "\n",
    "    #generate and share \"your\" unique hash\n",
    "    os.environ['USER_HASH'] = secrets.token_hex(nbytes=6)\n",
    "    print(f\"Your unique user hash is: {os.environ['USER_HASH']}\")\n",
    "\n",
    "    #get the current API key and combine with your hash\n",
    "    os.environ['OPENAI_API_KEY'] = f\"{os.environ['OPENAI_API_KEY']} {os.environ['USER_HASH']}\"\n",
    "else:\n",
    "    openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "    openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "openai.default_model = os.environ['OPENAI_API_ENGINE']\n",
    "\n",
    "import json\n",
    "# pretty printing JSON objects\n",
    "def json_pretty(input_object):\n",
    "  print(json.dumps(input_object, indent=4))\n",
    "\n",
    "\n",
    "import textwrap\n",
    "# wrap text when printing, because colab scrolls output to the right too much\n",
    "def wrap_text(text, width):\n",
    "    wrapped_text = textwrap.wrap(text, width)\n",
    "    return '\\n'.join(wrapped_text)\n",
    "\n",
    "def print_light_blue(text):\n",
    "    print(f'\\033[94m{text}\\033[0m')\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "class NotebookChatExperience:\n",
    "    def __init__(self, ai_response_function, ai_name = \"AI\"):\n",
    "        self.ai_name = ai_name\n",
    "        self.ai_response_function = ai_response_function\n",
    "        self.chat_history = widgets.Textarea(\n",
    "            value='',\n",
    "            placeholder='Chat history will appear here...',\n",
    "            description='Chat:',\n",
    "            disabled=True,\n",
    "            layout=widgets.Layout(width='700px', height='300px')  # Adjust the size as needed\n",
    "        )\n",
    "        self.user_input = widgets.Text(\n",
    "            value='',\n",
    "            placeholder='Type your message here...',\n",
    "            description='You:',\n",
    "            disabled=False,\n",
    "            layout=widgets.Layout(width='700px')  # Adjust the size as needed\n",
    "        )\n",
    "        self.user_input.on_submit(self.on_submit)\n",
    "        display(self.chat_history, self.user_input)\n",
    "\n",
    "    def on_submit(self, event):\n",
    "        user_message = self.user_input.value\n",
    "        ai_name = self.ai_name\n",
    "        self.chat_history.value += f\"\\nYou: {user_message}\"\n",
    "        ai_message = self.ai_response_function(user_message)\n",
    "        self.chat_history.value += f\"\\n{ai_name}: {ai_message}\"\n",
    "        self.user_input.value = ''  # Clear input for next message\n",
    "\n",
    "    def clear_chat(self):\n",
    "        self.chat_history.value = ''  # Clear the chat history\n",
    "\n",
    "## ********** Example usage:\n",
    "\n",
    "## ********** Define a simple AI response function\n",
    "# def simple_ai_response(user_message):\n",
    "    # return f\"AI > Echo: {user_message}\"\n",
    "\n",
    "## ********** Create an instance of the chat interface\n",
    "#chat_instance = NotebookChatExperience(simple_ai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## openai will look through environment variables itself to find the the OPENAI_API_KEY \n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai.api_key, base_url=openai.api_base)\n",
    "models_list = client.models.list().dict()\n",
    "\n",
    "# Convert the dictionary to a pretty-printed JSON string\n",
    "pretty_json = json.dumps(models_list, indent=4)\n",
    "\n",
    "print(pretty_json)\n",
    "\n",
    "## an error here is expected. the proxy may not implement all options the OpenAI does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the OpenAI ChatCompletion API\n",
    "def chatCompletion(messages, max_tokens=100):\n",
    "    client = OpenAI(api_key=openai.api_key, base_url=openai.api_base)\n",
    "    completion = client.chat.completions.create(\n",
    "        model=openai.default_model,\n",
    "        max_tokens=max_tokens,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "prompt=\"Hello, is ChatGPT online and working?\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "completion = chatCompletion(messages)\n",
    "\n",
    "response_text = completion.choices[0].message.content\n",
    "\n",
    "print(wrap_text(completion.model_dump_json(),70))\n",
    "\n",
    "print(\"\\n\", wrap_text(response_text,70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=openai.api_key, base_url=openai.api_base)\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What color is the sky?\"}\n",
    "  ],\n",
    "  max_tokens=100\n",
    ")\n",
    "\n",
    "\n",
    "print(wrap_text(completion.choices[0].message.content,70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_context = \"The color of the sky is purple today\"\n",
    "\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a helpful AI assistant that answers questions using the provided context.\n",
    "If you can't answer the question using the provided context, say \"I don't know\".\n",
    "Don't apologize, and don't say you are an AI assistant.\n",
    "\n",
    "Context: {retrieved_context}\n",
    "\"\"\"\n",
    "\n",
    "question = \"What is the sky like today?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\", \"content\": system_prompt},\n",
    "    {\"role\":\"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=messages,\n",
    "  max_tokens=100,\n",
    "  temperature=0,\n",
    ")\n",
    "\n",
    "\n",
    "print(wrap_text(completion.choices[0].message.content,70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_ai_response(user_message):\n",
    "  messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "  completion = chatCompletion(messages)\n",
    "  response_text = completion.choices[0].message.content\n",
    "  return response_text\n",
    "\n",
    "chat_instance = NotebookChatExperience(openai_ai_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
