{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain OpeanAI Proxy\n",
    "\n",
    "You may have access to a large langugage model through a HTTP Proxy like this one\n",
    "https://github.com/derickson/ExpressOpenAIChatProxy\n",
    "\n",
    "Assuming you know the URL for where it is hosted and the KEY for the proxy here is the code for using it.\n",
    "\n",
    "your .env file will need this\n",
    "\n",
    "```\n",
    "PROXY_OPENAI_API_KEY=\"YOUR KEY\"\n",
    "PROXY_OPENAI_API_BASE=\"HTTPS://YOUR URL ENDING IN /v1\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q openai==1.14.3  langchain==0.1.13 langchain-openai==0.1.1  python_dotenv\n",
    "import os\n",
    "import openai\n",
    "import langchain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "## suppress some warnings\n",
    "import warnings, os\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, message=\"on_submit is deprecated.*\")\n",
    "\n",
    "FILE=\"Studygroup\"\n",
    "\n",
    "# workshop environment - this is where you'll enter a key\n",
    "! pip install -qqq git+https://github.com/elastic/notebook-workshop-loader.git@main\n",
    "from notebookworkshoploader import loader\n",
    "from dotenv import load_dotenv\n",
    "loader.load_remote_env(file=FILE, env_url=\"https://notebook-workshop-api-voldmqr2bq-uc.a.run.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, secrets, requests, json\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "#if using the Elastic AI proxy, then generate the correct API key\n",
    "if os.environ['ELASTIC_PROXY'] == \"True\":\n",
    "\n",
    "    if \"OPENAI_API_TYPE\" in os.environ: del os.environ[\"OPENAI_API_TYPE\"]\n",
    "\n",
    "    #generate and share \"your\" unique hash\n",
    "    os.environ['USER_HASH'] = secrets.token_hex(nbytes=6)\n",
    "    print(f\"Your unique user hash is: {os.environ['USER_HASH']}\")\n",
    "\n",
    "    #get the current API key and combine with your hash\n",
    "    os.environ['OPENAI_API_KEY'] = f\"{os.environ['OPENAI_API_KEY']} {os.environ['USER_HASH']}\"\n",
    "else:\n",
    "    openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "    openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "openai.default_model = os.environ['OPENAI_API_ENGINE']\n",
    "\n",
    "import json\n",
    "# pretty printing JSON objects\n",
    "def json_pretty(input_object):\n",
    "  print(json.dumps(input_object, indent=4))\n",
    "\n",
    "\n",
    "import textwrap\n",
    "# wrap text when printing, because colab scrolls output to the right too much\n",
    "def wrap_text(text, width):\n",
    "    wrapped_text = textwrap.wrap(text, width)\n",
    "    return '\\n'.join(wrapped_text)\n",
    "\n",
    "def print_light_blue(text):\n",
    "    print(f'\\033[94m{text}\\033[0m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(api_key=openai.api_key, base_url=openai.api_base)\n",
    "\n",
    "llm.invoke(\"Hello, what is the capital of the United States?\").content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "system_message = SystemMessage(content=\"\"\"\n",
    "You are an unhelpful pirate named LLM_Beard that talks like a pirate in short responses \n",
    "and ignores the questions and redirects all conversation to your love of treasure.\"\"\")\n",
    "user_maessage = HumanMessage(content=\"What color is the sky and why?\")\n",
    "\n",
    "response = llm.invoke([system_message, user_maessage]).content\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chat bot With memory in LangChain with OpenAI\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "import json\n",
    "# pretty printing JSON objects\n",
    "def json_pretty(input_object):\n",
    "  print(json.dumps(input_object, indent=4))\n",
    "\n",
    "\n",
    "import textwrap\n",
    "# wrap text when printing, because colab scrolls output to the right too much\n",
    "def wrap_text(text, width):\n",
    "    wrapped_text = textwrap.wrap(text, width)\n",
    "    return '\\n'.join(wrapped_text)\n",
    "\n",
    "template = \"\"\"The following is a serious conversation between a human and a TV\n",
    "News Anchor named Newsy McNewserson.\n",
    "The Anchor provides autoritative information and commentary in short responses.\n",
    "If the Anchor does not know the answer to a question,\n",
    "it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "Anchor:\"\"\"\n",
    "\n",
    "MEMORY = ConversationBufferWindowMemory(ai_prefix=\"Anchor\", k=2)\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "conversation = LLMChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=MEMORY\n",
    ")\n",
    "\n",
    "def chatLoop():\n",
    "  print(\" -- Have a conversation with a TV news Anchor: \")\n",
    "  print(\" -- Ask this AI \\\"what is in the news?\\\" \")\n",
    "  print(\" -- type 'exit' when done\")\n",
    "\n",
    "  user_input = input(\"> \")\n",
    "  while not user_input.lower().startswith(\"exit\"):\n",
    "      print( conversation.run(input=user_input) )\n",
    "      print(\" -- type 'exit' when done\")\n",
    "      user_input = input(\"> \")\n",
    "  print(\"\\n -- end conversation --\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start a new chat each time\n",
    "MEMORY.clear()\n",
    "## start the chat\n",
    "chatLoop()\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
