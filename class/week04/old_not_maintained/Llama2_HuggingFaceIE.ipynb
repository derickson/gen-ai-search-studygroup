{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLama2 on Hugging Face Inference Endpoints\n",
    "\n",
    "Hugging Face runs a LLM hosting service that is pretty cheap if you are willing to swipe your own credit card.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How expensive is this to run here?\n",
    "\n",
    "![Cost](img/hfcost.jpg)\n",
    "\n",
    "$1.30 an hour.  I leave it on a setting that puts it to sleep after 15 minutes of inactivity\n",
    "\n",
    "### Setup\n",
    "\n",
    "From [https://huggingface.co/](https://huggingface.co/)\n",
    "* Create an account\n",
    "* set up a billing method for your account\n",
    "* LLama2 is commercially licensed, so you need to read and accept their agreeement before you can use the models.  This also means that if you are downloading the models locally (assuming you have a big enough computer) you will need to use your HuggingFace API BearerKey to be able to pull down the model files. Go to [a Llama2 Model Card](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) and go through the Access workflow.\n",
    "* With that done navigate in the Hugging Face menubar to Solutions > InferenceEndpoints\n",
    "* Create a new Endpoint\n",
    "    * Model repository : meta-llama/Llama-2-13b-chat-hf\n",
    "    * Cloud Provider: AWS\n",
    "    * Instance Type: GPU \\[medium\\] 1x Nvidia A10G\n",
    "    * Advanced Settings: \n",
    "        * Task  : Text Generation\n",
    "        * Automatic Scale-to-Zero : After 15 minutes with no activity\n",
    "\n",
    "## It can take 10 minutes to launch\n",
    "\n",
    "Once you start this thing up, it will take at leat 10 minutes warm up as the files are loaded into a GPU's VRAM\n",
    "\n",
    "You'll get an {'error': 'Bad Gateway'} response until the thing is 100% turned on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment variables\n",
    "\n",
    "Make sure your .env file has the following environment variable set\n",
    "\n",
    "```bash\n",
    "## for hosting Llama2 on Hugging Face\n",
    "HUGGINGFACEHUB_API_TOKEN=\"\"\n",
    "LLAMA2_HF_URL=\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Bad Gateway'}\n"
     ]
    }
   ],
   "source": [
    "## this is adapted from the code example from the bottom of the inference page\n",
    "! pip install -q python_dotenv\n",
    "! pip install -q requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "import requests\n",
    "\n",
    "API_URL = os.environ['LLAMA2_HF_URL']\n",
    "HUGGINGFACEHUB_API_TOKEN = os.environ['HUGGINGFACEHUB_API_TOKEN']\n",
    "headers = {\n",
    "\t\"Authorization\": f\"Bearer {HUGGINGFACEHUB_API_TOKEN}\",\n",
    "\t\"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"<s>[INST] <<SYS>>Complete the following: <</SYS>>The current prime minister of the United Kingdom is [/INST] \",\n",
    "})\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
