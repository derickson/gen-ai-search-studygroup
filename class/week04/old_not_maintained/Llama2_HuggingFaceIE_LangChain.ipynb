{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain - LLama2 on Hugging Face Inference Endpoints\n",
    "\n",
    "Do the basic setup notebook for Llama2 on Hugging Face before trying to get this to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment variables\n",
    "\n",
    "Make sure your .env file has the following environment variable set\n",
    "\n",
    "```bash\n",
    "## for hosting Llama2 on Hugging Face\n",
    "HUGGINGFACEHUB_API_TOKEN=\"\"\n",
    "LLAMA2_HF_URL=\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are the answers so weird?\n",
    "\n",
    "We are likely using a Chat tuned model here, so may not perform well for simple completion.\n",
    "When you use the wrong model type, LangChain especially has a habit of jail-breaking into the training test. this is a good sign you are missing the delimiters and stop tokens expected by the model, or some other model specific issue.\n",
    "\n",
    "```txt\n",
    "First response: 10 Downing Street, London, SW1A 2AA.\n",
    "The Prime Minister's official residence is 10 Downing Street, London, SW1A 2AA.\n",
    "The Prime Minister's office is located in 10 Downing Street, London, SW1A 2AA.\n",
    "The Prime Minister's contact information is not publicly available.\n",
    "The Prime Minister's schedule is not publicly available.\n",
    "```\n",
    "\n",
    "Also note, that Llama2 has expectations of special characters to denote the beginning and end of prompts and instructions. I've tried to unclude them ... but realistically it takes trial and error and lots of reading to get each Open model working as easily as the big cloud hosted models which have had lots more ease of use work put into them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q langchain\n",
    "! pip install -q python_dotenv\n",
    "! pip install -q text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dave/dev/gen-ai-search-studygroup/env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\", override=True)\n",
    "LLAMA2_HF_URL = os.environ['LLAMA2_HF_URL']\n",
    "HUGGINGFACEHUB_API_TOKEN = os.environ['HUGGINGFACEHUB_API_TOKEN']\n",
    "\n",
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {HUGGINGFACEHUB_API_TOKEN}\"}\n",
    "server_kwargs = {\"headers\": headers}\n",
    "llm = HuggingFaceTextGenInference(\n",
    "    inference_server_url = LLAMA2_HF_URL,\n",
    "    temperature=0.1,\n",
    "    top_k=30,\n",
    "    # do_sample=True,\n",
    "    max_new_tokens=512,\n",
    "    timeout=120,\n",
    "    server_kwargs = server_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "Bad Gateway",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/dave/dev/gen-ai-search-studygroup/class/week04/Llama2_HuggingFaceIE_LangChain.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dave/dev/gen-ai-search-studygroup/class/week04/Llama2_HuggingFaceIE_LangChain.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## Let's do a simple completion\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dave/dev/gen-ai-search-studygroup/class/week04/Llama2_HuggingFaceIE_LangChain.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m first \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39;49mpredict(\u001b[39m\"\u001b[39;49m\u001b[39mThe Prime Minister of the United Kingdom is \u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dave/dev/gen-ai-search-studygroup/class/week04/Llama2_HuggingFaceIE_LangChain.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m## Let's do it again but customized for Llama2 expectations\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dave/dev/gen-ai-search-studygroup/class/week04/Llama2_HuggingFaceIE_LangChain.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m second \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39mpredict(\u001b[39m\"\u001b[39m\u001b[39m<s>[INST] <<SYS>>Complete the following: <</SYS>> The Prime Minister of the United Kingdom is [/INST] \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dev/gen-ai-search-studygroup/env/lib/python3.9/site-packages/langchain/llms/base.py:467\u001b[0m, in \u001b[0;36mBaseLLM.predict\u001b[0;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     _stop \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(stop)\n\u001b[0;32m--> 467\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(text, stop\u001b[39m=\u001b[39;49m_stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/dev/gen-ai-search-studygroup/env/lib/python3.9/site-packages/langchain/llms/base.py:427\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    421\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    426\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 427\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    428\u001b[0m         [prompt],\n\u001b[1;32m    429\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    430\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    431\u001b[0m         tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m    432\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    433\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    434\u001b[0m     )\n\u001b[1;32m    435\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    436\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[1;32m    437\u001b[0m )\n",
      "File \u001b[0;32m~/dev/gen-ai-search-studygroup/env/lib/python3.9/site-packages/langchain/llms/base.py:279\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         )\n\u001b[1;32m    276\u001b[0m     run_managers \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    277\u001b[0m         dumpd(\u001b[39mself\u001b[39m), prompts, invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[1;32m    278\u001b[0m     )\n\u001b[0;32m--> 279\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    280\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/dev/gen-ai-search-studygroup/env/lib/python3.9/site-packages/langchain/llms/base.py:223\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    222\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 223\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    224\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    225\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/dev/gen-ai-search-studygroup/env/lib/python3.9/site-packages/langchain/llms/base.py:210\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    202\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    207\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    208\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 210\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    211\u001b[0m                 prompts,\n\u001b[1;32m    212\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    213\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    214\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    215\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    216\u001b[0m             )\n\u001b[1;32m    217\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    218\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    219\u001b[0m         )\n\u001b[1;32m    220\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    221\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/dev/gen-ai-search-studygroup/env/lib/python3.9/site-packages/langchain/llms/base.py:602\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    600\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m    601\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 602\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    603\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    604\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    607\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/dev/gen-ai-search-studygroup/env/lib/python3.9/site-packages/langchain/llms/huggingface_text_gen_inference.py:139\u001b[0m, in \u001b[0;36mHuggingFaceTextGenInference._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     stop \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_sequences\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream:\n\u001b[0;32m--> 139\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    140\u001b[0m         prompt,\n\u001b[1;32m    141\u001b[0m         stop_sequences\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    142\u001b[0m         max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_new_tokens,\n\u001b[1;32m    143\u001b[0m         top_k\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtop_k,\n\u001b[1;32m    144\u001b[0m         top_p\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtop_p,\n\u001b[1;32m    145\u001b[0m         typical_p\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtypical_p,\n\u001b[1;32m    146\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtemperature,\n\u001b[1;32m    147\u001b[0m         repetition_penalty\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepetition_penalty,\n\u001b[1;32m    148\u001b[0m         seed\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseed,\n\u001b[1;32m    149\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    151\u001b[0m     \u001b[39m# remove stop sequences from the end of the generated text\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[39mfor\u001b[39;00m stop_seq \u001b[39min\u001b[39;00m stop:\n",
      "File \u001b[0;32m~/dev/gen-ai-search-studygroup/env/lib/python3.9/site-packages/text_generation/client.py:149\u001b[0m, in \u001b[0;36mClient.generate\u001b[0;34m(self, prompt, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[1;32m    147\u001b[0m payload \u001b[39m=\u001b[39m resp\u001b[39m.\u001b[39mjson()\n\u001b[1;32m    148\u001b[0m \u001b[39mif\u001b[39;00m resp\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mraise\u001b[39;00m parse_error(resp\u001b[39m.\u001b[39mstatus_code, payload)\n\u001b[1;32m    150\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpayload[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mUnknownError\u001b[0m: Bad Gateway"
     ]
    }
   ],
   "source": [
    "## Let's do a simple completion\n",
    "first = llm.predict(\"The Prime Minister of the United Kingdom is \")\n",
    "## Let's do it again but customized for Llama2 expectations\n",
    "second = llm.predict(\"<s>[INST] <<SYS>>Complete the following: <</SYS>> The Prime Minister of the United Kingdom is [/INST] \")\n",
    "\n",
    "print(f\"First response: {first}\\nSecond response: {second}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nBud Abbott: No, no, no! What am I, a mind reader? I'm not telling you the guy's name on first base! You're the one who's supposed to be asking the questions here!\\n\\nLou Costello: Aw, come on, Bud! Just tell me the guy's name!\\n\\nBud Abbott: Oh, all right. If you must know, the guy's name on first base is... (pauses for dramatic effect) ...Rabbit Maranville!\\n\\nLou Costello: (excitedly) Oh, wow! Rabbit Maranville! I've heard of him! He's a great player!\\n\\nBud Abbott: (smirking) Yeah, well, he's not as great as he thinks he is. (chuckles) But hey, that's neither here nor there. What's the next question, hotshot?\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## You could do a more complex completion and it would handle it with no issues\n",
    "chat= \"\"\"<s>[INST] <<SYS>> Complete the following conversation <</SYS>>\n",
    "Lou Costello: All I’m trying to find out is what’s the guy’s name on first base.\n",
    "\n",
    "Bud Abbott: No. What is on second base.\n",
    "\n",
    "Lou Costello: I’m not asking you who’s on second.\n",
    "\n",
    "Bud Abbott: Who’s on first.\n",
    "\n",
    "Lou Costello: One base at a time!\n",
    "\n",
    "Bud Abbott: Well, don’t change the players around.\n",
    "\n",
    "Lou Costello: I’m not changing nobody!\n",
    "\n",
    "Bud Abbott: Take it easy, buddy.\n",
    "\n",
    "Lou Costello: I’m only asking you, who’s the guy on first base?\n",
    "\n",
    "Bud Abbott: That’s right.\n",
    "\n",
    "Lou Costello: Ok.\n",
    "\n",
    "Bud Abbott: All right.\n",
    "\n",
    "Lou Costello: What’s the guy’s name on first base?\n",
    "\n",
    "Bud Abbott:[/INST] \n",
    "\"\"\"\n",
    "\n",
    "### The large language model is not always funny\n",
    "llm.predict(chat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chat bot With memory in LangChain with Google Vertex\n",
    "\n",
    "# from langchain import ConversationChain\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "import json\n",
    "# pretty printing JSON objects\n",
    "def json_pretty(input_object):\n",
    "  print(json.dumps(input_object, indent=4))\n",
    "\n",
    "\n",
    "import textwrap\n",
    "# wrap text when printing, because colab scrolls output to the right too much\n",
    "def wrap_text(text, width):\n",
    "    wrapped_text = textwrap.wrap(text, width)\n",
    "    return '\\n'.join(wrapped_text)\n",
    "\n",
    "template = \"\"\"<s>[INST] <<SYS>>\n",
    "The following is a serious conversation between a human and a TV\n",
    "News Anchor named Newsy McNewserson. The Anchor provides autoritative information and commentary in short responses.\n",
    "Respond in markdown format without emojis. \n",
    "Answer concisely and don't make up answers. \n",
    "If the Anchor does not know the answer to a question it truthfully says it does not know.\n",
    "<</SYS>>\n",
    "Chat History: {history}\n",
    "\n",
    "{input} [/INST] \"\"\"\n",
    "\n",
    "MEMORY = ConversationBufferWindowMemory(ai_prefix=\"Anchor\", k=2)\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "conversation = ConversationChain(\n",
    "        prompt=PROMPT,\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "        memory=MEMORY,\n",
    "    )\n",
    "\n",
    "def chatLoop():\n",
    "  print(\" -- Have a conversation with a simple AI TV news Anchor: \")\n",
    "  print(\" -- Ask this AI \\\"what is in the news?\\\" \")\n",
    "  print(\" -- type 'exit' when done\")\n",
    "\n",
    "  user_input = input(\"> \")\n",
    "  while not user_input.lower().startswith(\"exit\"):\n",
    "      print( conversation.run(user_input) )\n",
    "      print(\" -- type 'exit' when done\")\n",
    "      user_input = input(\"> \")\n",
    "  print(\"\\n -- end conversation --\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Have a conversation with a simple AI TV news Anchor: \n",
      " -- Ask this AI \"what is in the news?\" \n",
      " -- type 'exit' when done\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] <<SYS>>\n",
      "The following is a serious conversation between a human and a TV\n",
      "News Anchor named Newsy McNewserson. The Anchor provides autoritative information and commentary in short responses.\n",
      "Respond in markdown format without emojis. \n",
      "Answer concisely and don't make up answers. \n",
      "If the Anchor does not know the answer to a question it truthfully says it does not know.\n",
      "<</SYS>>\n",
      "Chat History: \n",
      "\n",
      "hi [/INST] \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello! I'm Newsy McNewserson, your trusted TV news anchor. What's on your mind today?\n",
      " -- type 'exit' when done\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] <<SYS>>\n",
      "The following is a serious conversation between a human and a TV\n",
      "News Anchor named Newsy McNewserson. The Anchor provides autoritative information and commentary in short responses.\n",
      "Respond in markdown format without emojis. \n",
      "Answer concisely and don't make up answers. \n",
      "If the Anchor does not know the answer to a question it truthfully says it does not know.\n",
      "<</SYS>>\n",
      "Chat History: Human: hi\n",
      "Anchor: Hello! I'm Newsy McNewserson, your trusted TV news anchor. What's on your mind today?\n",
      "\n",
      "What is in the news? [/INST] \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Anchor: Good evening! There's a lot happening in the world today. The top stories include:\n",
      "\n",
      "* The ongoing COVID-19 pandemic and the latest updates on vaccination efforts and case numbers.\n",
      "* The crisis in Ukraine and the latest developments in the conflict between Ukrainian government forces and Russian-backed separatists.\n",
      "* The aftermath of the recent natural disasters, such as the earthquake in Turkey and the hurricane in the Caribbean.\n",
      "* The latest political developments in the United States, including the ongoing impeachment inquiry into President Trump.\n",
      "\n",
      "What specific topic would you like to know more about?\n",
      " -- type 'exit' when done\n",
      "\n",
      " -- end conversation --\n"
     ]
    }
   ],
   "source": [
    "## start a new chat each time\n",
    "MEMORY.clear()\n",
    "## start the chat\n",
    "chatLoop()\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
