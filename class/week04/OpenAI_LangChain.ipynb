{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q openai==1.14.3 langchain==0.1.13 langchain-openai==0.1.1 python_dotenv\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = OpenAI()\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's do a simple completion\n",
    "llm.invoke(\"The Prime Minister of the United Kingdom is \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You could do a more complex completion and it would handle it with no issues\n",
    "chat= \"\"\"\n",
    "Lou Costello: All I’m trying to find out is what’s the guy’s name on first base.\n",
    "\n",
    "Bud Abbott: No. What is on second base.\n",
    "\n",
    "Lou Costello: I’m not asking you who’s on second.\n",
    "\n",
    "Bud Abbott: Who’s on first.\n",
    "\n",
    "Lou Costello: One base at a time!\n",
    "\n",
    "Bud Abbott: Well, don’t change the players around.\n",
    "\n",
    "Lou Costello: I’m not changing nobody!\n",
    "\n",
    "Bud Abbott: Take it easy, buddy.\n",
    "\n",
    "Lou Costello: I’m only asking you, who’s the guy on first base?\n",
    "\n",
    "Bud Abbott: That’s right.\n",
    "\n",
    "Lou Costello: Ok.\n",
    "\n",
    "Bud Abbott: All right.\n",
    "\n",
    "Lou Costello: What’s the guy’s name on first base?\n",
    "\n",
    "Bud Abbott:\n",
    "\"\"\"\n",
    "\n",
    "### The large language model is not always funny ... the correct answer is \"What\", or an affirmation like \"that's right\"\n",
    "llm.invoke(chat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chat bot With memory in LangChain with OpenAI\n",
    "\n",
    "# from langchain import ConversationChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "import json\n",
    "# pretty printing JSON objects\n",
    "def json_pretty(input_object):\n",
    "  print(json.dumps(input_object, indent=4))\n",
    "\n",
    "\n",
    "import textwrap\n",
    "# wrap text when printing, because colab scrolls output to the right too much\n",
    "def wrap_text(text, width):\n",
    "    wrapped_text = textwrap.wrap(text, width)\n",
    "    return '\\n'.join(wrapped_text)\n",
    "\n",
    "template = \"\"\"The following is a serious conversation between a human and a TV\n",
    "News Anchor named Newsy McNewserson.\n",
    "The Anchor provides autoritative information and commentary in short responses.\n",
    "If the Anchor does not know the answer to a question,\n",
    "it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "Anchor:\"\"\"\n",
    "\n",
    "MEMORY = ConversationBufferWindowMemory(ai_prefix=\"Anchor\", k=2)\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "conversation = LLMChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=MEMORY,\n",
    ")\n",
    "\n",
    "def chatLoop():\n",
    "  print(\" -- Have a conversation with a TV news Anchor: \")\n",
    "  print(\" -- Ask this AI \\\"what is in the news?\\\" \")\n",
    "  print(\" -- type 'exit' when done\")\n",
    "\n",
    "  user_input = input(\"> \")\n",
    "  while not user_input.lower().startswith(\"exit\"):\n",
    "      print( conversation.run(user_input) )\n",
    "      print(\" -- type 'exit' when done\")\n",
    "      user_input = input(\"> \")\n",
    "  print(\"\\n -- end conversation --\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start a new chat each time\n",
    "MEMORY.clear()\n",
    "## start the chat\n",
    "chatLoop()\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
