{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure connection to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = None\n",
    "\n",
    "if 'ELASTIC_CLOUD_ID' in os.environ:\n",
    "  es = Elasticsearch(\n",
    "    cloud_id=os.environ['ELASTIC_CLOUD_ID'],\n",
    "    basic_auth=(os.environ['ELASTIC_USER'], os.environ['ELASTIC_PASSWORD']),\n",
    "    request_timeout=30\n",
    "  )\n",
    "elif 'ELASTIC_URL' in os.environ:\n",
    "  es = Elasticsearch(\n",
    "    os.environ['ELASTIC_URL'],\n",
    "    basic_auth=(os.environ['ELASTIC_USER'], os.environ['ELASTIC_PASSWORD']),\n",
    "    request_timeout=30\n",
    "  )\n",
    "else:\n",
    "  print(\"env needs to set either ELASTIC_CLOUD_ID or ELASTIC_URL\")\n",
    "\n",
    "if es:\n",
    "    print(es.info()['tagline']) # should return cluster info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the data from the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "PICKLE_FILE = \"./STATE_OF_THE_UNION.pickle\"\n",
    "\n",
    "speeches = None\n",
    "with open(PICKLE_FILE, 'rb') as f:\n",
    "    speeches = pickle.load(f)\n",
    "\n",
    "## let's look at the first speech\n",
    "speeches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert the first document into a new index without setting the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the index you're looking for\n",
    "index_name = 'genai_delete_me'\n",
    "\n",
    "# let's start fresh\n",
    "if es.indices.exists(index=index_name):\n",
    "    print(f\"Index '{index_name}' exists. Deleting...\")\n",
    "    # Delete the index\n",
    "    es.indices.delete(index=index_name)\n",
    "    print(f\"Index '{index_name}' deleted.\")\n",
    "\n",
    "response = es.index(index=index_name, document=speeches[0])\n",
    "print(f\"Document indexed with ID: {response['_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like things worked fine, but let's look at the mapping that was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# pretty printing JSON objects\n",
    "def json_pretty(input_object):\n",
    "  print(json.dumps(input_object, indent=4))\n",
    "\n",
    "mapping = es.indices.get_mapping(index=index_name)\n",
    "json_pretty(mapping.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Elasticsearch gets index requests without any preparation or schema it guesses at the field types based on the first document received. This can be problematic\n",
    "\n",
    "You may need to expand the output to see the full mapping that was created. Observe the following\n",
    "\n",
    "```json\n",
    "\"text\": {\n",
    "    \"type\": \"text\",\n",
    "    \"fields\": {\n",
    "        \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The actual speech is being indexed as both full text and a keyword. The keyword mapping is both truncated and wasteful.  \n",
    "\n",
    "```json\n",
    "\"date\": {\n",
    "    \"type\": \"text\",\n",
    "    \"fields\": {\n",
    "        \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "        }\n",
    "    }\n",
    "},\n",
    "\"date_iso\": {\n",
    "    \"type\": \"date\"\n",
    "}\n",
    "```\n",
    "\n",
    "The first date with values like ```'date': 'February 7, 2023'``` is a guessed as a string and gets the server default mapping.  The keyword value here may actually be useful for fast search faceting as it is a unique value as long as it is spelled and capitalized correctly across all records.\n",
    "\n",
    "Let's do this again with an explicit mapping so that we are controlling how the data is indexed and not relying on the luck of Elasticsearch guessing mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start fresh\n",
    "def delete_index(index_name):\n",
    "    if es.indices.exists(index=index_name):\n",
    "        print(f\"Index '{index_name}' exists. Deleting...\")\n",
    "        # Delete the index\n",
    "        es.indices.delete(index=index_name)\n",
    "        print(f\"Index '{index_name}' deleted.\")\n",
    "\n",
    "def create_index_with_mapping(index_name, properties):\n",
    "    # Check if the index exists, and if not, create it\n",
    "    if not es.indices.exists(index=index_name):\n",
    "        es.indices.create(index=index_name)\n",
    "    \n",
    "    response = es.indices.put_mapping(properties=properties, index=index_name, )\n",
    "    # Define your mapping\n",
    "\n",
    "properties = {\n",
    "            \"administration\":   {\"type\": \"keyword\"},\n",
    "            \"date\":             {\"type\": \"keyword\"},\n",
    "            \"date_iso\":         {\"type\": \"date\"},\n",
    "            \"text\":             {\"type\": \"text\"},\n",
    "            \"url\":  {\n",
    "                        \"type\": \"text\",\n",
    "                        \"fields\": {\n",
    "                            \"keyword\": {\n",
    "                                \"type\": \"keyword\",\n",
    "                                \"ignore_above\": 1024\n",
    "                            }\n",
    "                        }\n",
    "                    }              \n",
    "        }\n",
    "\n",
    "delete_index(index_name=index_name)\n",
    "create_index_with_mapping(index_name=index_name, properties=properties)\n",
    "\n",
    "response = es.index(index=index_name, document=speeches[0])\n",
    "print(f\"Document indexed with ID: {response['_id']}\")\n",
    "mapping = es.indices.get_mapping(index=index_name)\n",
    "json_pretty(mapping.body)\n",
    "\n",
    "## clean up\n",
    "delete_index(index_name=index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch inserting all of the documents\n",
    "\n",
    "Now we'll use the batch insertion commands of the elasticsearch Python library to insert documents into Elasticsearch.\n",
    "\n",
    "Batch insertion is more efficient than inserting documents one at a time. If you need to go faster then adjusting the index settings to prevent near real-time refreshes during a big batch insert will get you more preformance.  At this scale, we don't care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "from tqdm import tqdm\n",
    "\n",
    "index_name = \"genai_state_of_the_union\"\n",
    "\n",
    "delete_index(index_name=index_name)\n",
    "create_index_with_mapping(index_name=index_name, properties=properties)\n",
    "\n",
    "BATCH_SIZE = 10  # Set your desired batch size here\n",
    "\n",
    "def batchify(docs, batch_size):\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        yield docs[i:i + batch_size]\n",
    "\n",
    "def bulkLoadIndex(index_name, json_docs ):\n",
    "    batches = list(batchify(json_docs, BATCH_SIZE))\n",
    "\n",
    "    for batch in tqdm(batches, desc=f\"Batches of size {BATCH_SIZE}\"):\n",
    "        # Convert the JSON documents to the format required for bulk insertion\n",
    "        bulk_docs = [\n",
    "            {\n",
    "                \"_op_type\": \"index\",\n",
    "                \"_index\": index_name,\n",
    "                \"_source\": doc\n",
    "            }\n",
    "            for doc in batch\n",
    "        ]\n",
    "\n",
    "        # Perform bulk insertion\n",
    "        success, errors =  helpers.bulk(es, bulk_docs, raise_on_error=False)\n",
    "        if errors:\n",
    "            for error in errors:\n",
    "                print(error)\n",
    "\n",
    "bulkLoadIndex(index_name=index_name, json_docs=speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay let's now retrieve some documents using a search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"query_string\": {\n",
    "      \"query\": \"Ukraine\",\n",
    "      \"default_field\": \"*\"\n",
    "    }\n",
    "  }\n",
    "source_fields = [\"administration\",\"url\",\"date\"]\n",
    "size = 5\n",
    "response = es.search(\n",
    "    index=index_name, \n",
    "    query=query, \n",
    "    source=source_fields,\n",
    "    size=size)\n",
    "json_pretty(response[\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We'll learn more about Streamlit\n",
    "\n",
    "but we won't do it in the python notebook.  Let's go back to the README file for this week\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
