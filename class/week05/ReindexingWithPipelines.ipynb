{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reindexing with Ingest Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = None\n",
    "\n",
    "if 'ELASTIC_CLOUD_ID' in os.environ:\n",
    "  es = Elasticsearch(\n",
    "    cloud_id=os.environ['ELASTIC_CLOUD_ID'],\n",
    "    basic_auth=(os.environ['ELASTIC_USER'], os.environ['ELASTIC_PASSWORD']),\n",
    "    request_timeout=30\n",
    "  )\n",
    "elif 'ELASTIC_URL' in os.environ:\n",
    "  es = Elasticsearch(\n",
    "    os.environ['ELASTIC_URL'],\n",
    "    basic_auth=(os.environ['ELASTIC_USER'], os.environ['ELASTIC_PASSWORD']),\n",
    "    request_timeout=30\n",
    "  )\n",
    "else:\n",
    "  print(\"env needs to set either ELASTIC_CLOUD_ID or ELASTIC_URL\")\n",
    "\n",
    "if es:\n",
    "    print(es.info()['tagline']) # should return cluster info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions from week 2\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from tqdm import tqdm\n",
    "\n",
    "def delete_index(index_name):\n",
    "    if es.indices.exists(index=index_name):\n",
    "        print(f\"Index '{index_name}' exists. Deleting...\")\n",
    "        # Delete the index\n",
    "        es.indices.delete(index=index_name)\n",
    "        print(f\"Index '{index_name}' deleted.\")\n",
    "\n",
    "def create_index_with_mapping(index_name, properties, dynamic_templates=None):\n",
    "    # Check if the index exists, and if not, create it\n",
    "    if not es.indices.exists(index=index_name):\n",
    "        es.indices.create(index=index_name)\n",
    "    \n",
    "    if(dynamic_templates):\n",
    "        response = es.indices.put_mapping(properties=properties, index=index_name, dynamic_templates=dynamic_templates )\n",
    "    else:\n",
    "        response = es.indices.put_mapping(properties=properties, index=index_name )\n",
    "\n",
    "def batchify(docs, batch_size):\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        yield docs[i:i + batch_size]\n",
    "\n",
    "def bulkLoadIndex(index_name, json_docs ):\n",
    "    batches = list(batchify(json_docs, BATCH_SIZE))\n",
    "\n",
    "    for batch in tqdm(batches, desc=f\"Batches of size {BATCH_SIZE}\"):\n",
    "        # Convert the JSON documents to the format required for bulk insertion\n",
    "        bulk_docs = [\n",
    "            {\n",
    "                \"_op_type\": \"index\",\n",
    "                \"_index\": index_name,\n",
    "                \"_source\": doc\n",
    "            }\n",
    "            for doc in batch\n",
    "        ]\n",
    "\n",
    "        # Perform bulk insertion\n",
    "        success, errors =  helpers.bulk(es, bulk_docs, raise_on_error=False)\n",
    "        if errors:\n",
    "            for error in errors:\n",
    "                print(error)\n",
    "\n",
    "def changeEsRefreshInterval(es, index_name, refresh_interval):\n",
    "    body = {\n",
    "        \"index\": {\n",
    "            \"refresh_interval\": refresh_interval\n",
    "        }\n",
    "    }\n",
    "    response = es.indices.put_settings(index=index_name, body=body)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we are going to grab a dataset of trees in Washington DC\n",
    "\n",
    "[https://opendata.dc.gov/datasets/urban-forestry-street-trees/explore](https://opendata.dc.gov/datasets/urban-forestry-street-trees/explore)\n",
    "\n",
    "![geo download](img/geodownload.jpg)\n",
    "Download the latest data and put it in ```data/Urban_Forestry_Street_Trees.geojson```\n",
    "\n",
    "Now, let's loop through a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Load the GeoJSON file\n",
    "with open('data/Urban_Forestry_Street_Trees.geojson', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out a single record to take a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( data[\"features\"][0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll upload the data to a staging index as-is, with a simple mapping to control the indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "index_name = \"dc_urban_forestry_staging\"\n",
    "delete_index(index_name=index_name)\n",
    "\n",
    "dynamic_templates = [\n",
    "            {\"strings_as_keywords\":{\n",
    "                \"match_mapping_type\":\"string\",\n",
    "                \"mapping\": { \"type\": \"keyword\"}\n",
    "            }}\n",
    "        ]\n",
    "\n",
    "properties =  {\n",
    "        \"properties.CONDITIODT\": {     \"type\": \"date\"},\n",
    "        \"properties.LAST_EDITED_DATE\": {     \"type\": \"date\"},\n",
    "        \"properties.DATE_PLANT\": {   \"type\": \"date\"},\n",
    "        \"geometry\": {     \"type\": \"geo_shape\"}\n",
    "    }\n",
    "    \n",
    "\n",
    "create_index_with_mapping(index_name=index_name, \n",
    "                          properties=properties,\n",
    "                          dynamic_templates=dynamic_templates)\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "changeEsRefreshInterval(es,index_name=index_name,refresh_interval=\"30s\")\n",
    "bulkLoadIndex(index_name,data[\"features\"])\n",
    "\n",
    "# single_doc = data[\"features\"][0]\n",
    "# response = es.index(index=index_name, document=single_doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the data and set the refresh interval back to 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Geo goes faster if you force a merge\n",
    "response = es.indices.forcemerge(index=index_name, max_num_segments=1)\n",
    "changeEsRefreshInterval(es,index_name=index_name,refresh_interval=\"1s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting in Python Notebooks\n",
    "\n",
    "This data is now loaded and ready to look at with Kibana. Often, building apps on a data set should start with a browse around Kibana to see just how dirty the data is or what kinds of values we can find in the various fields.\n",
    "\n",
    "![Kibana](img/kibana.jpg)\n",
    "\n",
    "I noticed a few things. I wanted to learn where the oldest trees in DC were, but there is no AGE metric in the data. Generating a bucket with an **average** age requires a number, not a date.\n",
    "\n",
    "There is a DATE_PLANT field with the date of planting, but it is only populated for half the records. This is good place to learn how to do a transform on the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(es.count(index=index_name)[\"count\"], \"\\trecords in all\")\n",
    "\n",
    "print(es.count(index=index_name, query={\n",
    "    \"exists\": { \"field\": \"properties.DATE_PLANT\" }\n",
    "  })[\"count\"], \"\\trecords with a DATE_PLANT field\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that it is not populated consistently.  More than half the records are missing a plant date.\n",
    "\n",
    "We can plot data by running an aggregation and plotting the results in the Python Notebook itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qqq matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "index_name = \"dc_urban_forestry_staging\"\n",
    "today_date = datetime.now().strftime('%Y-%m-%dT00:00:00Z')\n",
    "\n",
    "query = {\n",
    "    \"range\": {\n",
    "        \"properties.DATE_PLANT\": {\n",
    "            \"gte\": \"1990-01-01T00:00:00Z\",\n",
    "            \"lt\": today_date\n",
    "        }\n",
    "    }\n",
    "}\n",
    "aggs = {\n",
    "    \"histogram_agg\": {\"date_histogram\":{\"field\":\"properties.DATE_PLANT\", \"calendar_interval\": \"year\"}}\n",
    "}\n",
    "results = es.search(index=index_name, aggs=aggs, size=0, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract bucket data\n",
    "buckets = results['aggregations']['histogram_agg']['buckets']\n",
    "dates = [bucket['key_as_string'][:4] for bucket in buckets]\n",
    "counts = [bucket['doc_count'] for bucket in buckets]\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(dates, counts, align='center')\n",
    "plt.title(\"Current trees by Date Planted\")\n",
    "plt.xlabel(\"Year of Planting\")\n",
    "plt.ylabel(\"Count of Trees\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline to generate Age\n",
    "\n",
    "Let's give ourselves the exercise of computing an age value using a pipeline. The best time to do this sort of data cleanup is on ingest. While this is trivial in Python, learning how to do it with an intermediate technology has operational advantages.  \n",
    "\n",
    "* full ETL tools are likely the best enterprise option, but have steeper adoption curves\n",
    "* Runtime fields put a query-time computational burden on Elasticsearch we can avoid.\n",
    "* Logstash ... is the old way of doing things and will require extracting data already loaded into Elasticsearch\n",
    "\n",
    "Ingest processors have the advantage of being easily insertable to existing streaming Elasticsearch ingest situations through addition of a \"pipeline\" parameter in addition to having an easy way to re-process data in Elasticsearch without having to first extract it back to your linux environment (which may require some heavy network performance and data exfil costs)\n",
    "\n",
    "Let's run the following ingest processor on a single record using the simulate command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"dc_urban_forestry_staging\"\n",
    "today_date = datetime.now().strftime('%Y-%m-%dT00:00:00Z')\n",
    "\n",
    "processors = [\n",
    "    {\"script\": {\n",
    "        \"lang\": \"painless\",\n",
    "        \"source\": \"\"\"\n",
    "String date_planted = ctx[\"properties\"][\"DATE_PLANT\"];\n",
    "if(date_planted != null){\n",
    "    ZonedDateTime now = ZonedDateTime.parse(\"\"\"+  f\"\\\"{today_date}\\\"\"   +\"\"\");\n",
    "    ZonedDateTime zdt = ZonedDateTime.parse(date_planted);\n",
    "    ctx[\"properties\"][\"AGE_DAYS\"]  = (now.getMillis() - zdt.getMillis()) / 86400000;\n",
    "}\n",
    "\"\"\"\n",
    "    }}\n",
    "]\n",
    "es.ingest.put_pipeline(id=\"tree_cleanup\", processors=processors)\n",
    "\n",
    "docs = [\n",
    "    {\n",
    "        \"_source\": data[\"features\"][0] \n",
    "    }\n",
    "]\n",
    "\n",
    "es.ingest.simulate(id='tree_cleanup', docs=docs).body[\"docs\"][0][\"doc\"][\"_source\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that working we can do queue up a reindex.  It will process in the background, but if we grab the task_id we can query to see when it is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_index(index_name=\"dc_urban_forestry\")\n",
    "\n",
    "dynamic_templates = [\n",
    "            {\"strings_as_keywords\":{\n",
    "                \"match_mapping_type\":\"string\",\n",
    "                \"mapping\": { \"type\": \"keyword\"}\n",
    "            }}\n",
    "        ]\n",
    "\n",
    "properties =  {\n",
    "        \"properties.CONDITIODT\": {     \"type\": \"date\"},\n",
    "        \"properties.LAST_EDITED_DATE\": {     \"type\": \"date\"},\n",
    "        \"properties.DATE_PLANT\": {   \"type\": \"date\"},\n",
    "        \"geometry\": {     \"type\": \"geo_shape\"}\n",
    "    }\n",
    "    \n",
    "\n",
    "create_index_with_mapping(index_name=\"dc_urban_forestry\", \n",
    "                          properties=properties,\n",
    "                          dynamic_templates=dynamic_templates)\n",
    "\n",
    "changeEsRefreshInterval(es,index_name=\"dc_urban_forestry\",refresh_interval=\"30s\")\n",
    "\n",
    "\n",
    "index_name = \"dc_urban_forestry_staging\"\n",
    "source = {\n",
    "    \"index\": index_name\n",
    "}\n",
    "dest_name = \"dc_urban_forestry\"\n",
    "dest = {\n",
    "    \"index\": dest_name,\n",
    "    \"pipeline\": 'tree_cleanup'\n",
    "}\n",
    "\n",
    "task_id = es.reindex(source=source, dest=dest, wait_for_completion=False)[\"task\"]\n",
    "task_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.client import TasksClient\n",
    "import time\n",
    "\n",
    "tasks = TasksClient(client=es)\n",
    "is_completed = False\n",
    "while not is_completed:\n",
    "    tasks_api = tasks.get(task_id=task_id)\n",
    "    is_completed = tasks_api[\"completed\"]\n",
    "    done_count = tasks_api[\"task\"][\"status\"][\"created\"]\n",
    "    total_count = tasks_api[\"task\"][\"status\"][\"total\"]\n",
    "    print(f\"Processing ... {done_count}/{total_count}\")\n",
    "    time.sleep(5)\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because it is geo data, let's do a force merge again and reset the refresh interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = es.indices.forcemerge(index=\"dc_urban_forestry\", max_num_segments=1)\n",
    "changeEsRefreshInterval(es,index_name=\"dc_urban_forestry\",refresh_interval=\"1s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now redraw the aggregation with our new field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "index_name = \"dc_urban_forestry\"\n",
    "today_date = datetime.now().strftime('%Y-%m-%dT00:00:00Z')\n",
    "\n",
    "query = {\n",
    "    \"range\": {\n",
    "        \"properties.DATE_PLANT\": {\n",
    "            \"gte\": \"1990-01-01T00:00:00Z\",\n",
    "            \"lt\": today_date\n",
    "        }\n",
    "    }\n",
    "}\n",
    "aggs = {\n",
    "    \"age_agg\": {\"histogram\":{\"field\":\"properties.AGE_DAYS\", \"interval\": 1}}\n",
    "}\n",
    "results = es.search(index=index_name, aggs=aggs, size=0, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract bucket data\n",
    "buckets = results['aggregations']['age_agg']['buckets']\n",
    "dates = [bucket['key'] for bucket in buckets]\n",
    "counts = [bucket['doc_count'] for bucket in buckets]\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(dates, counts, align='center')\n",
    "plt.title(\"Age of trees -- looks like DC sometimes plants 150 trees in a day!\")\n",
    "plt.xlabel(\"Age in days\")\n",
    "plt.ylabel(\"Count of Trees\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
