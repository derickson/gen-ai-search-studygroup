{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reindex State of the union with NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You Know, for Search\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = None\n",
    "\n",
    "if 'ELASTIC_CLOUD_ID' in os.environ:\n",
    "  es = Elasticsearch(\n",
    "    cloud_id=os.environ['ELASTIC_CLOUD_ID'],\n",
    "    basic_auth=(os.environ['ELASTIC_USER'], os.environ['ELASTIC_PASSWORD']),\n",
    "    request_timeout=30\n",
    "  )\n",
    "elif 'ELASTIC_URL' in os.environ:\n",
    "  es = Elasticsearch(\n",
    "    os.environ['ELASTIC_URL'],\n",
    "    basic_auth=(os.environ['ELASTIC_USER'], os.environ['ELASTIC_PASSWORD']),\n",
    "    request_timeout=30\n",
    "  )\n",
    "else:\n",
    "  print(\"env needs to set either ELASTIC_CLOUD_ID or ELASTIC_URL\")\n",
    "\n",
    "if es:\n",
    "    print(es.info()['tagline']) # should return cluster info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions from week 2\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from tqdm import tqdm\n",
    "\n",
    "def delete_index(index_name):\n",
    "    if es.indices.exists(index=index_name):\n",
    "        print(f\"Index '{index_name}' exists. Deleting...\")\n",
    "        # Delete the index\n",
    "        es.indices.delete(index=index_name)\n",
    "        print(f\"Index '{index_name}' deleted.\")\n",
    "\n",
    "def create_index_with_mapping(index_name, properties, dynamic_templates=None):\n",
    "    # Check if the index exists, and if not, create it\n",
    "    if not es.indices.exists(index=index_name):\n",
    "        es.indices.create(index=index_name)\n",
    "    \n",
    "    if(dynamic_templates):\n",
    "        response = es.indices.put_mapping(properties=properties, index=index_name, dynamic_templates=dynamic_templates )\n",
    "    else:\n",
    "        response = es.indices.put_mapping(properties=properties, index=index_name )\n",
    "\n",
    "def batchify(docs, batch_size):\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        yield docs[i:i + batch_size]\n",
    "\n",
    "def bulkLoadIndex(index_name, json_docs ):\n",
    "    batches = list(batchify(json_docs, BATCH_SIZE))\n",
    "\n",
    "    for batch in tqdm(batches, desc=f\"Batches of size {BATCH_SIZE}\"):\n",
    "        # Convert the JSON documents to the format required for bulk insertion\n",
    "        bulk_docs = [\n",
    "            {\n",
    "                \"_op_type\": \"index\",\n",
    "                \"_index\": index_name,\n",
    "                \"_source\": doc\n",
    "            }\n",
    "            for doc in batch\n",
    "        ]\n",
    "\n",
    "        # Perform bulk insertion\n",
    "        success, errors =  helpers.bulk(es, bulk_docs, raise_on_error=False)\n",
    "        if errors:\n",
    "            for error in errors:\n",
    "                print(error)\n",
    "\n",
    "def changeEsRefreshInterval(es, index_name, refresh_interval):\n",
    "    body = {\n",
    "        \"index\": {\n",
    "            \"refresh_interval\": refresh_interval\n",
    "        }\n",
    "    }\n",
    "    response = es.indices.put_settings(index=index_name, body=body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"genai_state_of_the_union\"\n",
    "source = es.search(index=index_name, size=1)[\"hits\"][\"hits\"][0][\"_source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Congressional](ORG&Congressional) [Record](MISC&Record), Volume 169 Issue 25 (Tuesday, February 7\n",
      "[{'entity': 'Congressional', 'class_name': 'ORG', 'class_probability': 0.9120655445160134, 'start_pos': 2, 'end_pos': 15}, {'entity': 'Record', 'class_name': 'MISC', 'class_probability': 0.5932554654550788, 'start_pos': 16, 'end_pos': 22}]\n"
     ]
    }
   ],
   "source": [
    "model_id= \"distilbert-base-cased-finetuned-conll03-english\"\n",
    "es_model_id = f\"elastic__{model_id}\"\n",
    "\n",
    "inference = {\n",
    "       \"inference\": {\n",
    "         \"model_id\": es_model_id,\n",
    "         \"field_map\": {\n",
    "           \"text\": \"text_field\"\n",
    "         }\n",
    "       }\n",
    "    }\n",
    "\n",
    "processors = [\n",
    "    inference\n",
    "\n",
    "]\n",
    "\n",
    "es.ingest.put_pipeline(id=\"sotu_ner\", processors=processors)\n",
    "\n",
    "docs = [\n",
    "    {\"_source\": source}\n",
    "]\n",
    "\n",
    "value = es.ingest.simulate(id='sotu_ner', docs=docs).body[\"docs\"][0][\"doc\"][\"_source\"]\n",
    "\n",
    "print(value[\"ml\"][\"inference\"][\"predicted_value\"][:100]) ### this is where the enrich text shows up\n",
    "\n",
    "print(value[\"ml\"][\"inference\"][\"entities\"][:2]) ### this is where the entities show up\n",
    "## creates new fields in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['date', 'ner_text', 'date_iso', 'administration', 'person', 'organization', 'location', 'text', 'url', 'misc'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_processor = {\"script\": {\n",
    "    \"lang\": \"painless\",\n",
    "    \"source\": \"\"\"\n",
    "\n",
    "Map convertMap = new HashMap();\n",
    "convertMap.put(\"PER\", \"person\");\n",
    "convertMap.put(\"ORG\", \"organization\");\n",
    "convertMap.put(\"MISC\", \"misc\");\n",
    "convertMap.put(\"LOC\", \"location\");\n",
    "\n",
    "ctx[\"ner_text\"] = ctx[\"ml\"][\"inference\"][\"predicted_value\"];\n",
    "for ( entity in  ctx[\"ml\"][\"inference\"][\"entities\"]){\n",
    "    String class_name = entity[\"class_name\"];\n",
    "    String key = convertMap.get(class_name);\n",
    "    String entity_value = entity[\"entity\"];\n",
    "    if (!ctx.containsKey(key)) {\n",
    "        ctx[key] = [];\n",
    "    }\n",
    "    if (! ctx[key].contains(entity_value)) {\n",
    "        ctx[key].add(entity_value);\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "}}\n",
    "\n",
    "remove_processor = { \"remove\": {\"field\": \"ml\"}}\n",
    "\n",
    "processors = [\n",
    "    inference,\n",
    "    script_processor,\n",
    "    remove_processor\n",
    "]\n",
    "\n",
    "es.ingest.put_pipeline(id=\"sotu_ner\", processors=processors)\n",
    "\n",
    "value = es.ingest.simulate(id='sotu_ner', docs=docs)\n",
    "\n",
    "value[\"docs\"][0][\"doc\"][\"_source\"].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'genai_state_of_the_union_ner' exists. Deleting...\n",
      "Index 'genai_state_of_the_union_ner' deleted.\n",
      "e56IHusaSEKA8xczV15eQg:16706069\n"
     ]
    }
   ],
   "source": [
    "destination_index = \"genai_state_of_the_union_ner\"\n",
    "delete_index(index_name=destination_index)\n",
    "\n",
    "dynamic_templates =  [\n",
    "      {\n",
    "        \"fields_with_prefix\": {\n",
    "          \"match_pattern\": \"regex\",\n",
    "          \"match\": \"^facet_.*\",\n",
    "          \"mapping\": {\n",
    "            \"type\": \"keyword\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "\n",
    "properties = {\n",
    "            \"administration\":   {\"type\": \"keyword\"},\n",
    "            \"date\":             {\"type\": \"keyword\"},\n",
    "            \"date_iso\":         {\"type\": \"date\"},\n",
    "            \"text\":             {\"type\": \"text\"},\n",
    "            \"person\":           {\"type\": \"keyword\"},\n",
    "            \"organization\":     {\"type\": \"keyword\"},\n",
    "            \"misc\":             {\"type\": \"keyword\"},\n",
    "            \"location\":         {\"type\": \"keyword\"},\n",
    "            \"ner_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"index\": False,\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"url\":  {\n",
    "                        \"type\": \"text\",\n",
    "                        \"fields\": {\n",
    "                            \"keyword\": {\n",
    "                                \"type\": \"keyword\",\n",
    "                                \"ignore_above\": 1024\n",
    "                            }\n",
    "                        }\n",
    "                    }              \n",
    "        }\n",
    "create_index_with_mapping(index_name=destination_index, \n",
    "                          properties=properties, \n",
    "                          dynamic_templates=dynamic_templates)\n",
    "\n",
    "index_name = \"genai_state_of_the_union\"\n",
    "source = {\n",
    "    \"index\": index_name\n",
    "}\n",
    "dest_name = destination_index\n",
    "dest = {\n",
    "    \"index\": dest_name,\n",
    "    \"pipeline\": 'sotu_ner'\n",
    "}\n",
    "\n",
    "task_id = es.reindex(source=source, dest=dest, wait_for_completion=False)[\"task\"]\n",
    "print(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ... 31/31\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch.client import TasksClient\n",
    "import time\n",
    "\n",
    "tasks = TasksClient(client=es)\n",
    "is_completed = False\n",
    "while not is_completed:\n",
    "    tasks_api = tasks.get(task_id=task_id)\n",
    "    is_completed = tasks_api[\"completed\"]\n",
    "    done_count = tasks_api[\"task\"][\"status\"][\"created\"]\n",
    "    total_count = tasks_api[\"task\"][\"status\"][\"total\"]\n",
    "    print(f\"Processing ... {done_count}/{total_count}\")\n",
    "    time.sleep(5)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
