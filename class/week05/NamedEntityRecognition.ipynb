{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Want to learn in way more detail what this is all about\n",
    "[https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a](https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q python_dotenv\n",
    "! pip install -q torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1\n",
    "! pip install -q seqeval==1.2.2\n",
    "! pip install -q transformers==4.39.1\n",
    "! pip install -q eland==8.12.1 sentence-transformers==2.6.1 requests==2.31.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESTART YOUR ENVIRONMENT.  \n",
    "\n",
    "This may be as simple as closing and relaunching visual studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id= \"distilbert-base-cased-finetuned-conll03-english\"\n",
    "es_model_id = f\"elastic__{model_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "\n",
    "\n",
    "domain=os.environ[\"ELASTIC_DOMAIN\"]\n",
    "port=os.environ[\"ELASTIC_PORT\"]\n",
    "protocol=os.environ[\"ELASTIC_PROTOCOL\"]\n",
    "user=os.environ[\"ELASTIC_USER\"]\n",
    "password=os.environ[\"ELASTIC_PASSWORD\"]\n",
    "\n",
    "es_url = f\"{protocol}://{user}:{password}@{domain}:{port}\"\n",
    "print(es_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "from pathlib import Path\n",
    "from eland.ml.pytorch import PyTorchModel\n",
    "from eland.ml.pytorch.transformers import TransformerModel\n",
    "import requests\n",
    "\n",
    "# Load a Hugging Face transformers model directly from the model hub\n",
    "tm = TransformerModel(model_id=f\"elastic/{model_id}\", task_type=\"ner\")\n",
    "\n",
    "\n",
    "# Export the model in a TorchScrpt representation which Elasticsearch uses\n",
    "tmp_path = \"models\"\n",
    "Path(tmp_path).mkdir(parents=True, exist_ok=True)\n",
    "model_path, config, vocab_path = tm.save(tmp_path)\n",
    "\n",
    "e = None\n",
    "\n",
    "# Import model into Elasticsearch\n",
    "es = elasticsearch.Elasticsearch(es_url, timeout=300)  # 5 minute timeout\n",
    "ptm = PyTorchModel(es, tm.elasticsearch_model_id())\n",
    "try:\n",
    "  ptm.import_model(model_path=model_path, config_path=None, vocab_path=vocab_path, config=config)\n",
    "except Exception as error:\n",
    "  # Handle the BadRequestError exception here\n",
    "  if error.meta.status == 400 and error.message == \"resource_already_exists_exception\":\n",
    "    print(\"Done -- the model was already loaded\")\n",
    "  else:\n",
    "    print(\"An error occurred:\", str(error))\n",
    "\n",
    "\n",
    "def deploy_model(model_id,es_url):\n",
    "  url = f\"{es_url}/_ml/trained_models/{model_id}/deployment/_start\"\n",
    "  response = requests.post(url)\n",
    "  if response.status_code == 200:\n",
    "    print(\"Model Deployed\")\n",
    "  else:\n",
    "    print(\"Error deploying model: \", response.text)\n",
    "\n",
    "deploy_model(es_model_id,es_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to deploy the model\n",
    "\n",
    "* If you are running in Elastic Cloud, make sure you have at least 1 ML node configured\n",
    "* In Kibana go to Stack Management > Machine Learning \n",
    "* Synchronize the saved objects\n",
    "\n",
    "![deploy](img/sync.jpg)\n",
    "\n",
    "* Refresh the page and go to the Trained Model Tab.\n",
    "* Start the model, default settings will be fine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = \"\"\"\n",
    "My name is Jonas\n",
    "I'm carryin' the wheel\n",
    "Thanks for all you've shown us\n",
    "But this is how we feel \"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def use_hosted_ml_post(json_payload, url):\n",
    "  json_string = json.dumps(json_payload)\n",
    "  headers = {'Content-Type': 'application/json'}\n",
    "  response = requests.post(es_url+url, data=json_string, headers=headers)\n",
    "  if response.status_code == 200:\n",
    "    print(\"JSON posted successfully to your URL\")\n",
    "    result = response.json()\n",
    "    print(json.dumps(result, indent=4))\n",
    "  else:\n",
    "    print(\"Error posting JSON:\", response.text)\n",
    "\n",
    "payload = {\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"text_field\": lyrics\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "url = f\"/_ml/trained_models/{es_model_id}/deployment/_infer\"\n",
    "use_hosted_ml_post(payload,url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = {\n",
    "       \"inference\": {\n",
    "         \"model_id\": es_model_id,\n",
    "         \"field_map\": {\n",
    "           \"message\": \"text_field\"\n",
    "         }\n",
    "       }\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "es.ingest.put_pipeline(id='week5_ner', processors=[inference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "  {\n",
    "      \"_source\": {\n",
    "          \"message\": lyrics\n",
    "      }\n",
    "  }\n",
    "]\n",
    "\n",
    "import json\n",
    "# pretty printing JSON objects\n",
    "def json_pretty(input_object):\n",
    "  print(json.dumps(input_object, indent=4))\n",
    "\n",
    "json_pretty(es.ingest.simulate(id='week5_ner', docs=docs).body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_extract = {\n",
    "       \"script\": {\n",
    "         \"lang\": \"painless\",\n",
    "         \"source\": \"\"\"\n",
    "String msg = ctx['message'];\n",
    "String predicted = ctx['ml']['inference']['predicted_value'];\n",
    "ctx['message_ner'] = ctx['ml']['inference']['predicted_value'];\n",
    "\"\"\"\n",
    "       }\n",
    "    }\n",
    "\n",
    "\n",
    "es.ingest.put_pipeline(id='week5_ner', processors = [inference, script_extract])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_pretty(es.ingest.simulate(id='week5_ner', docs=docs).body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = {\n",
    "       \"remove\": {\n",
    "         \"field\": [\n",
    "           \"ml\"\n",
    "         ],\n",
    "         \"ignore_missing\": True,\n",
    "         \"ignore_failure\": True\n",
    "       }\n",
    "    }\n",
    "\n",
    "\n",
    "es.ingest.put_pipeline(id='week5_ner', processors= [inference, script_extract,remove])\n",
    "json_pretty(es.ingest.simulate(id='week5_ner', docs=docs).body)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
